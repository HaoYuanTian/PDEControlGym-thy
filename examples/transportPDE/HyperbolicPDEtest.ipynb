{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b4d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "envs initialized\n"
     ]
    }
   ],
   "source": [
    "import pde_control_gym \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# 加载一些工具\n",
    "from utils import set_size\n",
    "from utils import linestyle_tuple\n",
    "from utils import load_csv\n",
    "# use the stable_baselines3 \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "# choose the pre-implemented reward function\n",
    "from pde_control_gym.src import TunedReward1D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd61164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NOISE\n",
    "# lambda state : state 输入状态值返回状态值\n",
    "def noiseFunc(state):\n",
    "    return state\n",
    "\n",
    "# Chebyshev Polynomial Beta Functions\n",
    "def solveBetaFunction(x, gamma):\n",
    "    # 先创建一个数组 shape =（len(x),）\n",
    "    beta = np.zeros(len(x), dtype=np.float32)\n",
    "    # 遍历赋值\n",
    "    for idx, val in enumerate(x):\n",
    "        # 在每一个离散点上计算beta\n",
    "        beta[idx] = 5 * math.cos(gamma * math.acos(val))\n",
    "    return beta\n",
    "\n",
    "# Returns beta functions passed into PDE environment. Currently gamma is always\n",
    "# set to 7.35, but this can be modified for further problems.\n",
    "# This function is used to create the beta function for the PDE environment.\n",
    "def getBetaFunction(nx):\n",
    "    return solveBetaFunction(np.linspace(0, 1, nx), 7.35)\n",
    "\n",
    "# Kernel function solver for backstepping\n",
    "def solveKernelFunction(beta, dx):\n",
    "    # theta 一个一维数组\n",
    "    # 创建一个和 theta 一样长度的数组 kappa\n",
    "    kappa = np.zeros(len(beta))\n",
    "    # 索引从 0 到 len（theta）- 1，总的长度还是 len（theta）\n",
    "    for i in range(0, len(beta)):\n",
    "        kernelIntegral = 0\n",
    "        # 矩形法离散积分函数值取左端点，所以只积分到 i-1 项\n",
    "        for j in range(0, i):\n",
    "            kernelIntegral += (kappa[i-j]*beta[j])*dx\n",
    "        kappa[i] = kernelIntegral  - beta[i]\n",
    "        # np.flip 用来翻转数组 （倒序 第一位为k（1））这样做是因为控制器是加权积分，（可以想象成卷积）因为状态是 u = [u(0), ····，u(1)]，对应的离散权重应该是 k =  [k(1), ····，u(0)]    \n",
    "        # 则U(1,t) = np.sum(u * k) 按位相乘再相加\n",
    "    return np.flip(kappa)\n",
    "\n",
    "# Control convolution solver\n",
    "def solveControl(kernel, u, dx):\n",
    "    res = 0\n",
    "    for i in range(len(u)):\n",
    "        res += kernel[i]*u[i]\n",
    "    return res*dx\n",
    "\n",
    "# Set initial condition function here\n",
    "def getInitialCondition(nx):\n",
    "    # *是按位乘法\n",
    "    return np.ones(nx)*np.random.uniform(1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd9d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep and spatial step for PDE Solver\n",
    "T = 5\n",
    "# 0.0001\n",
    "dt = 1e-4\n",
    "X = 1\n",
    "# 0.01\n",
    "dx = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc0e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义奖励函数：设置时间步、提前截断的单位时间步惩罚、正常终止的奖励\n",
    "reward_class =  TunedReward1D(int(round(T/dt)), -1e3, 3e2)\n",
    "\n",
    "# 先设置一个通用的参数字典\n",
    "hyperbolicParameters = {\n",
    "        \"T\": T, \n",
    "        \"dt\": dt, \n",
    "        \"X\": X,\n",
    "        \"dx\": dx, \n",
    "        \"reward_class\": reward_class,\n",
    "        \"normalize\":None, \n",
    "        \"sensing_loc\": \"full\", \n",
    "        \"control_type\": \"Dirchilet\", \n",
    "        \"sensing_type\": None,\n",
    "        # 确定传感器返回的测量值是否添加噪声，这里的这个表示精确的返回状态，并未添加任何噪声；（*lambda*构建了一个简单的函数，输入state，返回state）\n",
    "        \"sensing_noise_func\": lambda state: state,\n",
    "        # 用于早期停止的参数\n",
    "        \"limit_pde_state_size\": True,\n",
    "        \"max_state_value\": 1e10,\n",
    "        \"max_control_value\": 20,\n",
    "        # 传入初始条件的函数\n",
    "        \"reset_init_condition_func\": getInitialCondition,\n",
    "        # 传入计算beta的函数\n",
    "        \"reset_recirculation_func\": getBetaFunction,\n",
    "        # 控制采样频率 数值仿真时需要很小的时间步长，但控制器的接收控制信号无法这么快\n",
    "        \"control_sample_rate\": 0.1,\n",
    "}\n",
    "\n",
    "# All of the 1D PDE boundary control environments have the same set of optional parameters for ease of use! \n",
    "\n",
    "# 通过浅拷贝的方式设置 Backstepping 方法参数字典\n",
    "hyperbolicParametersBackstepping = hyperbolicParameters.copy()\n",
    "# 在复制过来的通用基础上修改某些关键参数 ⬇️ Normalize 专为强化学习控制器设计，如果设置为True，则控制器的动作值会被归一化到[-1, 1]之间,并根据\"max_control_value\"转换为实际控制值\n",
    "hyperbolicParametersBackstepping[\"normalize\"] = False\n",
    "\n",
    "# 设置 Rl 参数字典\n",
    "hyperbolicParametersRL = hyperbolicParameters.copy()\n",
    "# 需要用到线性化\n",
    "hyperbolicParametersRL[\"normalize\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963c50e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab422/anaconda3/envs/TxlRL/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# 设置路径 （algo_name 和 run 需要更改）\n",
    "algo_name = \"PPO\"\n",
    "run = 1\n",
    "run_id = f\"{algo_name}_run{run}\"\n",
    "log_dir = f\"./logs/{run_id}\"\n",
    "# print(run_id) \n",
    "# print(log_dir)\n",
    "model_path = os.path.join(log_dir, \"final_model.zip\")\n",
    "vecnorm_path = os.path.join(log_dir, \"vecnormalize.pkl\")\n",
    "\n",
    "# =====创建环境=======\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(\"PDEControlGym-TransportPDE1D\", **hyperbolicParametersRL))])\n",
    "\n",
    "# =====加载VecNormalize状态=====\n",
    "eval_env = VecNormalize.load(vecnorm_path, eval_env)\n",
    "eval_env.training = False     # 禁止在测试时更新统计量\n",
    "eval_env.norm_reward = False  # 还原 reward 为原始量纲（如需要）\n",
    "\n",
    "# ===加载模型===\n",
    "model_class = {\"PPO\": PPO, \"SAC\": SAC}[algo_name]\n",
    "model = model_class.load(model_path, env=eval_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TxlRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
